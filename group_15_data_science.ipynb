{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "8QCVSIePgILq"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from sklearn.pipeline import Pipeline\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "UHEZueSIgRs6",
        "outputId": "eca0659a-f901-47b1-d8d7-26ddd5bb1c76"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Group 15 data science project\\nDATASET:\\n(https://www.kaggle.com/competitions/playground-series-s4e11/data)\\n\\nPROCESS:\\n1. Load and clean the data\\n2. EDA - Exploring data\\n3. Data Pre-processing\\n4. Creating model\\n5. Cross-validated training\\n6. Evaluation of model\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "\"\"\"Group 15 data science project\n",
        "DATASET:\n",
        "(https://www.kaggle.com/competitions/playground-series-s4e11/data)\n",
        "\n",
        "PROCESS:\n",
        "1. Load and clean the data\n",
        "2. EDA - Exploring data\n",
        "3. Data Pre-processing\n",
        "4. Creating model\n",
        "5. Cross-validated training\n",
        "6. Evaluation of model\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "eqqFvOoTgDu6"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "1. Loading and cleaning data\n",
        "\n",
        "TODO:\n",
        "We need a better way of handling NA values in certain columns - KNN encoding?\n",
        "Run the DataAnalyser object before preprocessing (if verbose=True) such that it\n",
        "can present information about the data before we start altering it\n",
        "\"\"\"\n",
        "cleaning_transformations = [\n",
        "    {'drop': ['id', 'Name', 'City']},\n",
        "    {'handle_na': []}\n",
        "]\n",
        "\n",
        "# Define Cleaner Class\n",
        "class Cleaner(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    This class will be used as part of the ML Pipeline for cleaning and handling data\n",
        "    \"\"\"\n",
        "    def __init__(self, transformations):\n",
        "        self.transformations = transformations\n",
        "\n",
        "    def fit(self, x, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, x):\n",
        "        x_temp = x.copy()\n",
        "\n",
        "        for transformation in self.transformations:\n",
        "            for transformation_type, column_names in transformation.items():\n",
        "                if transformation_type == 'handle_na':\n",
        "                    x_temp = self.deal_with_na_values(x_temp)\n",
        "                elif transformation_type == 'drop':\n",
        "                    x_temp = self.drop_unneeded_columns(x_temp, column_names)\n",
        "\n",
        "        return x_temp\n",
        "\n",
        "    @staticmethod\n",
        "    def deal_with_na_values(data, verbose=False):\n",
        "        data.fillna(0, inplace=True)\n",
        "        return data\n",
        "\n",
        "    @staticmethod\n",
        "    def drop_unneeded_columns(data, column_names):\n",
        "        return data.drop(columns=column_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "XoXvVqRahumr",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "2. EDA - This is the class that will allow us to display information about the data\n",
        "         The idea is that this information that we can extract/plot will allow us to tweak out data-preprocessing pipeline\n",
        "         to achieve better and more generalisable results\n",
        "\n",
        "         TODO:\n",
        "         Assess the overall distribution of 'Depressed' and 'not depressed' individuals in the dataset\n",
        "         Confimatory factor analysis - or another method of figuring out how we can best assess each variables affect in building up\n",
        "                                       a model which can generalise and help predict which variables affect the overall ability of the model\n",
        "         look on kaggle for methods other users have used to\n",
        "\"\"\"\n",
        "\n",
        "class DataAnalyser:\n",
        "\n",
        "    def __init__(self, base_train_df):\n",
        "        self.base_train_df = base_train_df\n",
        "\n",
        "    def view_cleaning_analysis(self):\n",
        "        self.view_unique_columns()\n",
        "\n",
        "    def view_data_analysis(self):\n",
        "      self.component_analysis\n",
        "\n",
        "\n",
        "    # Cleaning analysis functions\n",
        "    def view_unique_columns(self):\n",
        "        print('[] Breakdown of Binary columns values []')\n",
        "        for column in self.base_train_df.columns:\n",
        "            print(f'Unique values in column \"{column}\": {self.base_train_df[column].unique()}')\n",
        "\n",
        "\n",
        "    # EDA anaylsis functions\n",
        "    def component_analysis(self):\n",
        "      ...\n",
        "\n",
        "# analyser = DataAnalyser(train_df)\n",
        "# analyser.view_cleaning_analysis()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "cbcCgqZwhwLj"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "3. Data Pre-Processing\n",
        "\n",
        "COLUMNS -\n",
        "id: Can be dropped\n",
        "Name: Can be dropped\n",
        "Gender: binary encoding\n",
        "age: Normalised\n",
        "city: Can be dropped\n",
        "Working professional or Student: Binary encoding\n",
        "Profession: Frequency-encoding\n",
        "Academic Pressure: Normalised\n",
        "Work Pressure: Normalised\n",
        "CGPA: Normalised\n",
        "Study Satisfaction: Normalised\n",
        "Job Satisfaction: Normalised\n",
        "Sleep Duration: mapped to values and normalised\n",
        "Dietary Habits: mapped to values and normalised\n",
        "Degree: Frequency encoding\n",
        "Suicidal Thoughts: Binary Encoding\n",
        "Work/Study Hours: Normalised\n",
        "Financial stress: Normalised\n",
        "Mental Illness: Binary Encoding\n",
        "Depression: Target variable (no pre-processing needed)\n",
        "\n",
        "TODO:\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "preprocessing_transformations = [\n",
        "    {'binary_encode': ['Gender', 'Have you ever had suicidal thoughts ?',\n",
        "                       'Family History of Mental Illness',\n",
        "                       'Working Professional or Student']},\n",
        "    {'map_sleep_column': ['Sleep Duration']},\n",
        "    {'map_diet_column': ['Dietary Habits']},\n",
        "    {'frequency_encode': ['Profession', 'Degree']},\n",
        "    {'normalise': ['Financial Stress', 'Age', 'Academic Pressure', 'Work Pressure',\n",
        "                   'CGPA', 'Study Satisfaction', 'Job Satisfaction', 'Work/Study Hours', 'Financial Stress']}\n",
        "]\n",
        "\n",
        "\n",
        "class PreProcessor(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    This class will handle the Pre-Processin of our data before passing to the Model\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, transformations):\n",
        "        self.transformations = transformations\n",
        "        self.mappings = {\n",
        "            'Gender': {'Male': 0, 'Female': 1},\n",
        "            'Have you ever had suicidal thoughts ?': {'Yes': 0, 'No': 1},\n",
        "            'Family History of Mental Illness': {'Yes': 0, 'No': 1},\n",
        "            'Working Professional or Student': {'Working Professional': 0, 'Student': 1},\n",
        "        }\n",
        "        self.sleep_mapping = {\n",
        "        \"More than 8 hours\":9,\n",
        "        'Less than 5 hours':4,\n",
        "        '5-6 hours':5.5,\n",
        "        '7-8 hours':7.5,\n",
        "        '1-2 hours':1.5,\n",
        "        '6-8 hours':7,\n",
        "        '4-6 hours':5,\n",
        "        '6-7 hours':6.5,\n",
        "        '10-11 hours':10.5,\n",
        "        '8-9 hours':8.5,\n",
        "        '9-11 hours':10,\n",
        "        '2-3 hours':2.5,\n",
        "        '3-4 hours':3.5,\n",
        "        'Moderate':6,\n",
        "        '4-5 hours':4.5,\n",
        "        '9-6 hours':7.5,\n",
        "        '1-3 hours':2,\n",
        "        '1-6 hours':4,\n",
        "        '8 hours':8,\n",
        "        '10-6 hours':8,\n",
        "        'Unhealthy':3,\n",
        "        'Work_Study_Hours':6,\n",
        "        '3-6 hours':4.5,\n",
        "        '9-5':7,\n",
        "        '9-5 hours':7,\n",
        "        }\n",
        "        self.diet_mapping ={\n",
        "            'More Healty':0,\n",
        "            'Healthy':1,\n",
        "            'Less than Healthy':2,\n",
        "            'Less Healthy':2,\n",
        "            'Moderate':3,\n",
        "            'Unhealthy':4,\n",
        "            'No Healthy':4,\n",
        "        }\n",
        "\n",
        "    def fit(self, x, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, x):\n",
        "        x_temp = x.copy()\n",
        "\n",
        "        for transformation in self.transformations:\n",
        "            for transformation_name, column_names in transformation.items():\n",
        "                if transformation_name == 'drop_columns':\n",
        "                    x_temp = self.drop_columns(x_temp, column_names)\n",
        "                elif transformation_name == 'binary_encode':\n",
        "                    x_temp = self.encode_columns(x_temp, column_names, self.mappings)\n",
        "                elif transformation_name == 'normalise':\n",
        "                    x_temp = self.normalise_columns(x_temp, column_names)\n",
        "                elif transformation_name == 'map_sleep_column':\n",
        "                    x_temp = self.map_sleep_values(x_temp, column_names, self.sleep_mapping)\n",
        "                elif transformation_name == 'frequency_encode':\n",
        "                    x_temp = self.frequency_encode(x_temp, column_names)\n",
        "                elif transformation_name == 'map_diet_column':\n",
        "                    x_temp = self.map_dietary_value(x_temp, column_names, self.diet_mapping)\n",
        "\n",
        "        return x_temp\n",
        "\n",
        "    @staticmethod\n",
        "    def drop_columns(data, column_names):\n",
        "        data.drop(columns=column_names, inplace=True)\n",
        "        return data\n",
        "\n",
        "    @staticmethod\n",
        "    def encode_columns(data, column_names, mappings):\n",
        "        for column in column_names:\n",
        "            if column in mappings:\n",
        "                data[column] = data[column].map(mappings[column])\n",
        "        return data\n",
        "\n",
        "    @staticmethod\n",
        "    def map_sleep_values(data, column_names, mapping):\n",
        "        for column in column_names:\n",
        "            data[column] = data[column].map(mapping)\n",
        "            data[column] = data[column].fillna(data[column].mode()[0])\n",
        "        return data\n",
        "\n",
        "    @staticmethod\n",
        "    def map_dietary_value(data, column_names, mapping):\n",
        "        for column in column_names:\n",
        "            data[column] = data[column].map(mapping)\n",
        "            data[column] = data[column].fillna(data[column].mode()[0])\n",
        "        return data\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def frequency_encode(data, column_names):\n",
        "        for column in column_names:\n",
        "            if column in data.columns:\n",
        "                freq = data[column].value_counts() / len(data)\n",
        "                data[column] = data[column].map(freq)\n",
        "        return data\n",
        "\n",
        "    @staticmethod\n",
        "    def one_hot_encode(data, column_names):\n",
        "        for column in column_names:\n",
        "            if column in data.columns:\n",
        "                one_hot = pd.get_dummies(data[column], prefix=column)\n",
        "                data = pd.concat([data, one_hot], axis=1)\n",
        "                data.drop(columns=[column], inplace=True)\n",
        "        return data\n",
        "\n",
        "    @staticmethod\n",
        "    def normalise_columns(data, column_names):\n",
        "        scaler = MinMaxScaler()  # Initialize MinMaxScaler\n",
        "        for column in column_names:\n",
        "            if column in data.columns:\n",
        "                # Reshape for MinMaxScaler since it expects 2D input\n",
        "                data[column] = scaler.fit_transform(data[[column]])\n",
        "        return data\n",
        "\n",
        "# data_pipeline = Pipeline([\n",
        "#      ('cleaning', Cleaner(cleaning_transformations)),\n",
        "#      ('preprocessing', PreProcessor(preprocessing_transformations))\n",
        "# ])\n",
        "\n",
        "# Apply the transformations to the data\n",
        "# processed_data = data_pipeline.transform(train_df)\n",
        "\n",
        "# # View the processed data\n",
        "# print(processed_data.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "sQ-OrqqibQNg"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "4. Creating Model / Training / Evaluating\n",
        "\n",
        "TODO:\n",
        "save TP, TN, FP, FN values for each iteration, save all values and average them\n",
        "\"\"\"\n",
        "\n",
        "# Define model parameters\n",
        "logistic_regression_params = {'max_iter': 1000, 'random_state': 42}\n",
        "decision_tree_params = {'criterion': 'gini', 'max_depth': 3, 'random_state': 42}\n",
        "random_forest_params = {'n_estimators': 100, 'max_depth': 5, 'random_state': 42}\n",
        "svm_params = {'kernel': 'linear', 'C': 1.0}\n",
        "knn_params = {'n_neighbors': 5}\n",
        "\n",
        "\n",
        "# Define models\n",
        "models = {'Logistic_regression': LogisticRegression(**logistic_regression_params),\n",
        "          'decision_tree': DecisionTreeClassifier(**decision_tree_params),\n",
        "          'random_forest': RandomForestClassifier(**random_forest_params)}\n",
        "# models.append({'SVM': SVC(**svm_params)})\n",
        "# models.append({'K-Nearest Neighbors': KNeighborsClassifier(**knn_params)})\n",
        "\n",
        "class Trainer(BaseEstimator, TransformerMixin):\n",
        "\n",
        "  def __init__(self, models, n_splits=5):\n",
        "    self.models = models\n",
        "    self.n_splits = n_splits\n",
        "    self.results = {}\n",
        "    self.results_analyser = ResultsAnalyser()\n",
        "\n",
        "  def fit(self,x,y):\n",
        "    skf = StratifiedKFold(n_splits=self.n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "    for model_name, model in self.models.items():\n",
        "      accuracies = []\n",
        "      tp_percentages, tn_percentages, fp_percentages, fn_percentages = [], [], [], []\n",
        "      for train_idx, val_idx in skf.split(x, y):\n",
        "        x_train, x_val = x.iloc[train_idx], x.iloc[val_idx]\n",
        "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
        "\n",
        "        model.fit(x_train, y_train)\n",
        "        y_pred = model.predict(x_val)\n",
        "        accuracy = accuracy_score(y_val, y_pred)\n",
        "        # accuracies.append(accuracy_score(y_val, y_pred))\n",
        "        tn, fp, fn, tp = confusion_matrix(y_val, y_pred).ravel()\n",
        "\n",
        "        # Total samples in fold\n",
        "        total = len(y_val)\n",
        "\n",
        "        # Store percentages\n",
        "        tp_percentages.append(tp / total)\n",
        "        tn_percentages.append(tn / total)\n",
        "        fp_percentages.append(fp / total)\n",
        "        fn_percentages.append(fn / total)\n",
        "        accuracies.append(accuracy)\n",
        "\n",
        "      self.results[model_name] = {\n",
        "        'accuracy': np.mean(accuracies),\n",
        "        'tp%': np.mean(tp_percentages),\n",
        "        'tn%': np.mean(tn_percentages),\n",
        "        'fp%': np.mean(fp_percentages),\n",
        "        'fn%': np.mean(fn_percentages)}\n",
        "    return self\n",
        "\n",
        "  def transform(self,x):\n",
        "    return x\n",
        "\n",
        "  def get_results(self):\n",
        "    self.results_analyser.view_results(self.results)\n",
        "    print(self.results)\n",
        "# results_analyser = ResultsAnalyser(model_results)\n",
        "# results_analyser.view_results()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "4Ihdisz3HiM4"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "5. Analysing results\n",
        "'''\n",
        "\n",
        "class ResultsAnalyser:\n",
        "    def __init__(self):\n",
        "      ...\n",
        "\n",
        "    def view_results(self, results):\n",
        "        for result in results.items():\n",
        "            print(result)\n",
        "\n",
        "    def create_confusion_matrix(fp,fn,tp,tn):\n",
        "      ...\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EI-qUZ0hHiM4",
        "outputId": "1985602d-6197-4e35-a93c-3ea6c7adf481"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('Logistic_regression', {'accuracy': np.float64(0.9376830135039091), 'tp%': np.float64(0.1462046908315565), 'tn%': np.float64(0.7914783226723525), 'fp%': np.float64(0.026808813077469795), 'fn%': np.float64(0.03550817341862118)})\n",
            "('decision_tree', {'accuracy': np.float64(0.9100995024875622), 'tp%': np.float64(0.15526652452025586), 'tn%': np.float64(0.7548329779673064), 'fp%': np.float64(0.063454157782516), 'fn%': np.float64(0.02644633972992182)})\n",
            "('random_forest', {'accuracy': np.float64(0.9275053304904052), 'tp%': np.float64(0.13539445628997868), 'tn%': np.float64(0.7921108742004265), 'fp%': np.float64(0.02617626154939588), 'fn%': np.float64(0.04631840796019901)})\n",
            "{'Logistic_regression': {'accuracy': np.float64(0.9376830135039091), 'tp%': np.float64(0.1462046908315565), 'tn%': np.float64(0.7914783226723525), 'fp%': np.float64(0.026808813077469795), 'fn%': np.float64(0.03550817341862118)}, 'decision_tree': {'accuracy': np.float64(0.9100995024875622), 'tp%': np.float64(0.15526652452025586), 'tn%': np.float64(0.7548329779673064), 'fp%': np.float64(0.063454157782516), 'fn%': np.float64(0.02644633972992182)}, 'random_forest': {'accuracy': np.float64(0.9275053304904052), 'tp%': np.float64(0.13539445628997868), 'tn%': np.float64(0.7921108742004265), 'fp%': np.float64(0.02617626154939588), 'fn%': np.float64(0.04631840796019901)}}\n"
          ]
        }
      ],
      "source": [
        "# Load the data\n",
        "dataset = pd.read_csv('train.csv')\n",
        "\n",
        "# Removing target Column\n",
        "target = dataset.pop('Depression')\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('cleaning', Cleaner(cleaning_transformations)),\n",
        "    ('preprocessor', PreProcessor(preprocessing_transformations)),\n",
        "    ('training', Trainer(models))\n",
        "])\n",
        "\n",
        "pipeline.fit(dataset, target)\n",
        "\n",
        "model_results = pipeline.named_steps['training'].get_results()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}